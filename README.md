**Xize Cheng (æˆæ›¦æ³½)** is a Third-Year Master's student **(expected to graduate at 2024.03)** in the College of Computer Science and Software at [Zhejiang University](https://www.zju.edu.cn/english/), supervised by [Prof. Zhou Zhao](https://person.zju.edu.cn/zhaozhou).

**I am actively looking for academic collaboration, feel free to drop me an email.**

# ğŸ“  HomePage

- Personal Pages: [https://exgc.github.io/](https://exgc.github.io/) (updated recentlyğŸ”¥)
- Google Scholar: [https://scholar.google.com.sg/citations?user=7w1U0l4AAAAJ](https://scholar.google.com.sg/citations?user=7w1U0l4AAAAJ)
  
# ğŸ”¥ News
- *2023.10*: ğŸ‰ğŸ‰ I am awarded National Scholarship (2023, Grauate student). Top 0.1% in Zhejiang University.
- *2023.09*: ğŸ‰ğŸ‰ 1 paper is accepted by EMNLP 2023!
- *2023.09*: ğŸ‰ğŸ‰ 1 paper is accepted by NIPS 2023!
- *2023.07*: ğŸ‰ğŸ‰ 1 Paper are accepted by ACMMM 2023!
- *2023.05*: ğŸ‰ğŸ‰ 3 Paper are accepted by ICCV 2023!
- *2023.06*: AV-TranSpeech comes out! Media coverage: [PaperWeekly](https://mp.weixin.qq.com/s/2KD8CYToz-mLZStwCXcSnA) and [ByteDance](https://mp.weixin.qq.com/s/SMUWbGqtyYRK6I_VW18hjA).
- *2023.05*: OpenSR will be presented in **oral** presentation at ACL2023!
- *2023.05*: ğŸ‰ğŸ‰ 7 Paper are accepted by ACL 2023!
- *2023.03*: We create the first Audio-Visual Multi-lingual Speech Translation dataset [AVMuST-TED ![](https://img.shields.io/github/stars/Exgc/AVMuST-TED?style=social)](https://github.com/Exgc/AVMuST-TED)! Soon to be open source!
- *2022.12*: [OpenSR](https://github.com/Exgc/OpenSR) is well regarded by the reviewers at October 2022 ACL-ARR.
- *2022.10*: I award the Outstanding Graduate Student and Triple Excellence Graduate Student of Zhejiang University!
- *2021.03*: I start my internship at Taobao as an algorithm intern, conducting multi-modality research.

# ğŸ“ Publications 

## Audio-Visual Speech

- [TransFace: Unit-Based Audio-Visual Speech Synthesizer for Talking Head Translation.]() **Xize Cheng**, Rongjie Huang, Linjun Li, Tao Jin, Zehan Wang, Aoxiong Yin, Minglei Li, Xinyu Duan, changpeng yang, Zhou Zhao. **submitted to ICLR2024**

- [MixSpeech: Cross-Modality Self-Learning with Audio-Visual Stream Mixup for Visual Speech Translation and Recognition.](https://arxiv.org/abs/2303.05309) **Xize Cheng**, Tao Jin, Rongjie Huang, Linjun Li, Wang Lin, Zehan Wang, Huadai Liu, Ye Wang, Aoxiong Yin, Zhou Zhao. **ICCV2023**

- [OpenSR: Open-Modality Speech Recognition via Maintaining Multi-Modality Alignment.](https://arxiv.org/abs/2306.06410) **Xize Cheng**, Tao Jin, Linjun Li, Wang Lin, Xinyu Duan, Zhou Zhao. **ACL2023(Oral)**

- [AV-TranSpeech: Audio-Visual Robust Speech-to-Speech Translation.](https://arxiv.org/abs/2305.15403) Rongjie Haung\*, **Xize Cheng**\*, Huadai Liu\*, Yi Ren, Linjun Li, Zhenhui Ye, Jinzheng He, Lichao Zhang, Jinglin Liu, Xiang Yin, Zhou Zhao. **ACL2023**

- [Contrastive Token-Wise Meta-Learning for Unseen Performer Visual Temporal-Aligned Translation.](https://aclanthology.org/2023.findings-acl.699/) Linjun Li\*, Tao Jin\*, **Xize Cheng**\*, Ye Wang, Wang Lin, Rongjie Huang and Zhou Zhao. **ACL2023**

- [Rethinking Missing Modality Learning from a Decoding Perspective.](https://dl.acm.org/doi/abs/10.1145/3581783.3612291) Tao Jin, **Xize Cheng**, Linjun Li, Wang Lin, Ye Wang, Zhou Zhao. **ACMMM2023**
  
